{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/blanca/.pyenv/versions/3.9.18/envs/myenv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import re\n",
    "\n",
    "# import streamlit as st\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "\n",
    "\n",
    "TOKENIZER_PATH = \"models/tokenizer.pickle\"\n",
    "\n",
    "MODELS = [\n",
    "    {\n",
    "        \"name\": \"Recurrent Neural Network\",\n",
    "        \"path\": \"models/rnn.h5\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Recurrent Neural Network with GloVe embeddings\",\n",
    "        \"path\": \"models/rnn_glove.h5\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Convolutional Neural Network\",\n",
    "        \"path\": \"models/cnn_model.keras\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Convolutional Neural Network with GloVe embeddings\",\n",
    "        \"path\": \"models/cnn_model_glove.keras\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Logistic Regression with Bag-of-Words\",\n",
    "        \"path\": \"models/logReg_bow.joblib\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Logistic Regression with TF-IDF\",\n",
    "        \"path\": \"models/logReg_tfidf.joblib\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Naive Bayes with TF-IDF\",\n",
    "        \"path\": \"models/naive_tfidf.joblib\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Naive Bayes with Bag-of-Words\",\n",
    "        \"path\": \"models/naive_tfidf.joblib\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DenseNet\",\n",
    "        \"path\": \"models/denseNet_model.h5\"\n",
    "    },\n",
    "]\n",
    "\n",
    "emotion_to_emoji = {\n",
    "    'admiration': 'ü§©',\n",
    "    'amusement': 'üòÑ',\n",
    "    'anger': 'üò°',\n",
    "    'annoyance': 'üòë',\n",
    "    'approval': 'üëç',\n",
    "    'caring': 'ü•∞',\n",
    "    'confusion': 'üòï',\n",
    "    'curiosity': 'ü§î',\n",
    "    'desire': 'üòè',\n",
    "    'disappointment': 'üòû',\n",
    "    'disapproval': 'üëé',\n",
    "    'disgust': 'ü§¢',\n",
    "    'embarrassment': 'üò≥',\n",
    "    'excitement': 'üòÉ',\n",
    "    'fear': 'üò®',\n",
    "    'gratitude': 'üôè',\n",
    "    'joy': 'üòÄ',\n",
    "    'love': '‚ù§Ô∏è',\n",
    "    'neutral': 'üòê',\n",
    "    'optimism': 'üòä',\n",
    "    'realization': 'üò≤',\n",
    "    'sadness': 'üò¢',\n",
    "    'surprise': 'üòÆ'\n",
    "}\n",
    "\n",
    "def load_tokenizer():\n",
    "    with open(TOKENIZER_PATH, \"rb\") as file:\n",
    "        tokenizer = pickle.load(file)\n",
    "    return tokenizer\n",
    "\n",
    "def remove_special_characters(sentence, remove_digits=False):\n",
    "    print(f'Removing special characters from sentence: {sentence}')\n",
    "    pattern = r'/[^\\w-]|_/' if not remove_digits else r'[^a-zA-Z\\s]'  \n",
    "    clean_text = re.sub(pattern, '', sentence)\n",
    "    print(f'Cleaned sentence: {clean_text}')\n",
    "    return clean_text\n",
    "\n",
    "def preprocess_input(text, maxlen=18):\n",
    "    # Download the NLTK resources and initialize the lemmatizer\n",
    "    nltk.download(\"stopwords\")\n",
    "    nltk.download(\"wordnet\")\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Remove special characters\n",
    "    text = remove_special_characters(text, remove_digits=True)\n",
    "\n",
    "    # Apply lemmatization\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    # Remove stopwords\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    # Text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize the input text\n",
    "    tokenizer = load_tokenizer()\n",
    "    text = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "    # Pad the input text\n",
    "    text = pad_sequences(text, maxlen=maxlen)\n",
    "\n",
    "    return text\n",
    "\n",
    "def predict_sentiment(text, model_used):\n",
    "    print(f\"Predicting sentiment for tokenized text: {text}\")\n",
    "    prediction = model_used.predict(text)[0]\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    # prediction is a list of probabilities for each class\n",
    "    # return the top 3 classes with the highest probabilities\n",
    "    # as well as the corresponding emojis\n",
    "    # as a list of tuples [(emoji, emotion, probability), ...]\n",
    "    top_classes = prediction.argsort()[-3:][::-1]\n",
    "    print(f\"Top classes: {top_classes}\")\n",
    "    emotion_labels = list(emotion_to_emoji.keys())\n",
    "    print(emotion_labels)\n",
    "    top_classes_info = [(emotion_to_emoji[emotion_labels[top_class]], emotion_labels[top_class], prediction[top_class]) for top_class in top_classes]\n",
    "    print(top_classes_info)\n",
    "    return top_classes_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import UnpicklingError\n",
    "\n",
    "def load_model_joblib(model_path):\n",
    "    model_loaded = joblib.load(model_path)\n",
    "    return model_loaded\n",
    "\n",
    "def predict_sentiment_joblib(text, model_used):\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    print(f'text: {text}')\n",
    "    print(f\"Predicting sentiment for tokenized text: {text}\")\n",
    "    prediction = model_used.predict_proba(text)[0]\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    # prediction is a list of probabilities for each class\n",
    "    # return the top 3 classes with the highest probabilities\n",
    "    # as well as the corresponding emojis\n",
    "    # as a list of tuples [(emoji, emotion, probability), ...]\n",
    "    # Obtener las clases del modelo\n",
    "    classes = model_used.classes_\n",
    "    print(\"Classes:\", classes)\n",
    "\n",
    "    # Encontrar los √≠ndices de las tres mayores probabilidades\n",
    "    top_indices = prediction.argsort()[-3:][::-1]\n",
    "    print(f\"Top classes indices: {top_indices}\")\n",
    "\n",
    "    # Recolectar la informaci√≥n de las tres mejores clases\n",
    "    top_classes_info = [(emotion_to_emoji[classes[i]], classes[i], prediction[i]) for i in top_indices]\n",
    "    print(\"Top classes information:\", top_classes_info)\n",
    "\n",
    "    return top_classes_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "def main(text):\n",
    "\n",
    "    # Dropdown for selecting the model\n",
    "    model_selected = \"Convolutional Neural Network with GloVe embeddings\"\n",
    "\n",
    "    # Load the selected model\n",
    "    model_path = [model[\"path\"] for model in MODELS if model[\"name\"] == model_selected][0]\n",
    "    print(f'Loading model from path: {model_path}')\n",
    "    if model_path.endswith(\".joblib\"):\n",
    "        model_loaded_joblib = joblib.load(model_path)\n",
    "        print(f'Model loaded: {model_loaded_joblib}')\n",
    "        result = predict_sentiment_joblib(text, model_loaded_joblib)\n",
    "    else:\n",
    "        model_loaded = load_model(model_path)\n",
    "        print(f'Model loaded: {model_loaded}')\n",
    "        if \"Convolutional\" in model_selected:\n",
    "            text = preprocess_input(text, maxlen=19)\n",
    "        else:\n",
    "            text = preprocess_input(text, maxlen=18)\n",
    "        # Predict the sentiment\n",
    "        result = predict_sentiment(text, model_loaded)\n",
    "        # Display the sentiment analysis results\n",
    "    for emotion in result:\n",
    "        print(f\"Emoji: {emotion[0]}, Emotion: {emotion[1]}, Probability: {emotion[2]}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from path: models/cnn_model_glove.keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: <keras.src.engine.sequential.Sequential object at 0x131e27220>\n",
      "Removing special characters from sentence: I am so happy today\n",
      "Cleaned sentence: I am so happy today\n",
      "Predicting sentiment for tokenized text: [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1 119\n",
      "  286]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/blanca/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/blanca/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 148ms/step\n",
      "Prediction: [1.93475056e-02 9.83530562e-03 3.87222245e-02 8.94250199e-02\n",
      " 9.16875675e-02 2.40979642e-02 2.35543773e-02 2.14404929e-02\n",
      " 3.45964858e-18 6.94395741e-03 4.08765748e-02 5.81160821e-02\n",
      " 3.15549485e-02 1.37034655e-02 1.09884795e-02 9.98274889e-03\n",
      " 3.63386027e-03 1.29760895e-02 3.71070555e-03 3.84331554e-01\n",
      " 1.88727807e-02 4.43505235e-02 3.01145017e-02 1.17333299e-02]\n",
      "Top classes: [19  4  3]\n",
      "['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'joy', 'love', 'neutral', 'optimism', 'realization', 'sadness', 'surprise']\n",
      "[('üòä', 'optimism', 0.38433155), ('üëç', 'approval', 0.09168757), ('üòë', 'annoyance', 0.08942502)]\n",
      "Emoji: üòä, Emotion: optimism, Probability: 0.3843315541744232\n",
      "Emoji: üëç, Emotion: approval, Probability: 0.09168756753206253\n",
      "Emoji: üòë, Emotion: annoyance, Probability: 0.08942501991987228\n"
     ]
    }
   ],
   "source": [
    "# Run the app\n",
    "text = \"I am so happy today\"\n",
    "if __name__ == \"__main__\":\n",
    "    if len(text)>18:\n",
    "        main(text)\n",
    "    else:\n",
    "        print(\"Text must be at least 18 characters long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
