{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\belus\\MasterBD\\ADNE\\sentimentAnalysis\\SentAnalysis\\Scripts\\python.exe\n",
      "WARNING:tensorflow:From c:\\Users\\belus\\MasterBD\\ADNE\\sentimentAnalysis\\SentAnalysis\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import re\n",
    "\n",
    "# import streamlit as st\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "\n",
    "\n",
    "TOKENIZER_PATH = \"models/tokenizer.pickle\"\n",
    "\n",
    "MODELS = [\n",
    "    {\n",
    "        \"name\": \"Recurrent Neural Network\",\n",
    "        \"path\": \"models/rnn.h5\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Recurrent Neural Network with GloVe embeddings\",\n",
    "        \"path\": \"models/rnn_glove.h5\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Convolutional Neural Network\",\n",
    "        \"path\": \"models/cnn_model.keras\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Convolutional Neural Network with GloVe embeddings\",\n",
    "        \"path\": \"models/cnn_model_glove.keras\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Long Short Term Memory Network\",\n",
    "        \"path\": \"models/lstm_l2_model.keras\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Long Short Term Memory Networks with GloVe embeddings\",\n",
    "        \"path\": \"models/lstm_glove_model.keras\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Logistic Regression with Bag-of-Words\",\n",
    "        \"path\": \"models/logReg_bow.joblib\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Logistic Regression with TF-IDF\",\n",
    "        \"path\": \"models/logReg_tfidf.joblib\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Naive Bayes with TF-IDF\",\n",
    "        \"path\": \"models/naive_tfidf.joblib\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Naive Bayes with Bag-of-Words\",\n",
    "        \"path\": \"models/naive_tfidf.joblib\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DenseNet\",\n",
    "        \"path\": \"models/denseNet_model.h5\"\n",
    "    },\n",
    "]\n",
    "\n",
    "emotion_to_emoji = {\n",
    "    'admiration': 'ü§©',\n",
    "    'amusement': 'üòÑ',\n",
    "    'anger': 'üò°',\n",
    "    'annoyance': 'üòë',\n",
    "    'approval': 'üëç',\n",
    "    'caring': 'ü•∞',\n",
    "    'confusion': 'üòï',\n",
    "    'curiosity': 'ü§î',\n",
    "    'desire': 'üòè',\n",
    "    'disappointment': 'üòû',\n",
    "    'disapproval': 'üëé',\n",
    "    'disgust': 'ü§¢',\n",
    "    'embarrassment': 'üò≥',\n",
    "    'excitement': 'üòÉ',\n",
    "    'fear': 'üò®',\n",
    "    'gratitude': 'üôè',\n",
    "    'joy': 'üòÄ',\n",
    "    'love': '‚ù§Ô∏è',\n",
    "    'neutral': 'üòê',\n",
    "    'optimism': 'üòä',\n",
    "    'realization': 'üò≤',\n",
    "    'sadness': 'üò¢',\n",
    "    'surprise': 'üòÆ'\n",
    "}\n",
    "\n",
    "def load_tokenizer():\n",
    "    with open(TOKENIZER_PATH, \"rb\") as file:\n",
    "        tokenizer = pickle.load(file)\n",
    "    return tokenizer\n",
    "\n",
    "def remove_special_characters(sentence, remove_digits=False):\n",
    "    print(f'Removing special characters from sentence: {sentence}')\n",
    "    pattern = r'/[^\\w-]|_/' if not remove_digits else r'[^a-zA-Z\\s]'  \n",
    "    clean_text = re.sub(pattern, '', sentence)\n",
    "    print(f'Cleaned sentence: {clean_text}')\n",
    "    return clean_text\n",
    "\n",
    "def preprocess_input(text, maxlen=18):\n",
    "    # Download the NLTK resources and initialize the lemmatizer\n",
    "    nltk.download(\"stopwords\")\n",
    "    nltk.download(\"wordnet\")\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Remove special characters\n",
    "    text = remove_special_characters(text, remove_digits=True)\n",
    "\n",
    "    # Apply lemmatization\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    # Remove stopwords\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    # Text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize the input text\n",
    "    tokenizer = load_tokenizer()\n",
    "    text = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "    # Pad the input text\n",
    "    text = pad_sequences(text, maxlen=maxlen)\n",
    "\n",
    "    return text\n",
    "\n",
    "def predict_sentiment(text, model_used):\n",
    "    print(f\"Predicting sentiment for tokenized text: {text}\")\n",
    "    prediction = model_used.predict(text)[0]\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    # prediction is a list of probabilities for each class\n",
    "    # return the top 3 classes with the highest probabilities\n",
    "    # as well as the corresponding emojis\n",
    "    # as a list of tuples [(emoji, emotion, probability), ...]\n",
    "    top_classes = prediction.argsort()[-3:][::-1]\n",
    "    print(f\"Top classes: {top_classes}\")\n",
    "    emotion_labels = list(emotion_to_emoji.keys())\n",
    "    print(emotion_labels)\n",
    "    top_classes_info = [(emotion_to_emoji[emotion_labels[top_class]], emotion_labels[top_class], prediction[top_class]) for top_class in top_classes]\n",
    "    print(top_classes_info)\n",
    "    return top_classes_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import UnpicklingError\n",
    "\n",
    "def load_model_joblib(model_path):\n",
    "    model_loaded = joblib.load(model_path)\n",
    "    return model_loaded\n",
    "\n",
    "def predict_sentiment_joblib(text, model_used):\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    print(f'text: {text}')\n",
    "    print(f\"Predicting sentiment for tokenized text: {text}\")\n",
    "    prediction = model_used.predict_proba(text)[0]\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    # prediction is a list of probabilities for each class\n",
    "    # return the top 3 classes with the highest probabilities\n",
    "    # as well as the corresponding emojis\n",
    "    # as a list of tuples [(emoji, emotion, probability), ...]\n",
    "    # Obtener las clases del modelo\n",
    "    classes = model_used.classes_\n",
    "    print(\"Classes:\", classes)\n",
    "\n",
    "    # Encontrar los √≠ndices de las tres mayores probabilidades\n",
    "    top_indices = prediction.argsort()[-3:][::-1]\n",
    "    print(f\"Top classes indices: {top_indices}\")\n",
    "\n",
    "    # Recolectar la informaci√≥n de las tres mejores clases\n",
    "    top_classes_info = [(emotion_to_emoji[classes[i]], classes[i], prediction[i]) for i in top_indices]\n",
    "    print(\"Top classes information:\", top_classes_info)\n",
    "\n",
    "    return top_classes_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "def main(text):\n",
    "\n",
    "    # Dropdown for selecting the model\n",
    "    model_selected = \"Long Short Term Memory Network\"\n",
    "\n",
    "    # Load the selected model\n",
    "    model_path = [model[\"path\"] for model in MODELS if model[\"name\"] == model_selected][0]\n",
    "    print(f'Loading model from path: {model_path}')\n",
    "    if model_path.endswith(\".joblib\"):\n",
    "        model_loaded_joblib = joblib.load(model_path)\n",
    "        print(f'Model loaded: {model_loaded_joblib}')\n",
    "        result = predict_sentiment_joblib(text, model_loaded_joblib)\n",
    "    else:\n",
    "        model_loaded = load_model(model_path)\n",
    "        print(f'Model loaded: {model_loaded}')\n",
    "        if \"Convolutional\" or \"Long\" in model_selected:\n",
    "            text = preprocess_input(text, maxlen=19)\n",
    "        else:\n",
    "            text = preprocess_input(text, maxlen=18)\n",
    "        # Predict the sentiment\n",
    "        result = predict_sentiment(text, model_loaded)\n",
    "        # Display the sentiment analysis results\n",
    "    for emotion in result:\n",
    "        print(f\"Emoji: {emotion[0]}, Emotion: {emotion[1]}, Probability: {emotion[2]}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from path: models/lstm_l2_model.keras\n",
      "WARNING:tensorflow:From c:\\Users\\belus\\MasterBD\\ADNE\\sentimentAnalysis\\SentAnalysis\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\belus\\MasterBD\\ADNE\\sentimentAnalysis\\SentAnalysis\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model loaded: <keras.src.engine.sequential.Sequential object at 0x0000023A03EDB9D0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\belus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\belus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing special characters from sentence: I am so happy today\n",
      "Cleaned sentence: I am so happy today\n",
      "Predicting sentiment for tokenized text: [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1 119\n",
      "  286]]\n",
      "1/1 [==============================] - 1s 542ms/step\n",
      "Prediction: [0.05278483 0.01616734 0.01522265 0.0407591  0.10318875 0.03381812\n",
      " 0.04512066 0.05519021 0.02222641 0.02736776 0.03291195 0.00654131\n",
      " 0.00771095 0.03468016 0.00712719 0.00844997 0.01945665 0.00680149\n",
      " 0.31418985 0.04308632 0.05460414 0.01141269 0.04118155]\n",
      "Top classes: [18  4  7]\n",
      "['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'joy', 'love', 'neutral', 'optimism', 'realization', 'sadness', 'surprise']\n",
      "[('üòê', 'neutral', 0.31418985), ('üëç', 'approval', 0.10318875), ('ü§î', 'curiosity', 0.055190213)]\n",
      "Emoji: üòê, Emotion: neutral, Probability: 0.3141898512840271\n",
      "Emoji: üëç, Emotion: approval, Probability: 0.10318875312805176\n",
      "Emoji: ü§î, Emotion: curiosity, Probability: 0.05519021302461624\n"
     ]
    }
   ],
   "source": [
    "# Run the app\n",
    "text = \"I am so happy today\"\n",
    "if __name__ == \"__main__\":\n",
    "    if len(text)>18:\n",
    "        main(text)\n",
    "    else:\n",
    "        print(\"Text must be at least 18 characters long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
